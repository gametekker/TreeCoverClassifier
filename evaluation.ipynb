{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "sns.set_theme(context=\"notebook\", style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = Path(\"models/random_forest.joblib\")\n",
        "if not model_path.exists():\n",
        "    raise FileNotFoundError(\"Missing models/random_forest.joblib. Run train.ipynb first.\")\n",
        "\n",
        "rf_model = joblib.load(model_path)\n",
        "\n",
        "test_df = pd.read_csv(\"preprocessed_test.csv.gz\")\n",
        "X_test = test_df.drop(\"Cover_Type\", axis=1)\n",
        "y_test = test_df[\"Cover_Type\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "metrics_df = pd.DataFrame(\n",
        "    {\n",
        "        \"Metric\": [\n",
        "            \"Accuracy\",\n",
        "            \"Precision (weighted)\",\n",
        "            \"Precision (macro)\",\n",
        "            \"Recall (weighted)\",\n",
        "            \"Recall (macro)\",\n",
        "            \"F1 (weighted)\",\n",
        "            \"F1 (macro)\",\n",
        "        ],\n",
        "        \"Score\": [\n",
        "            accuracy_score(y_test, y_pred),\n",
        "            precision_score(y_test, y_pred, average=\"weighted\"),\n",
        "            precision_score(y_test, y_pred, average=\"macro\"),\n",
        "            recall_score(y_test, y_pred, average=\"weighted\"),\n",
        "            recall_score(y_test, y_pred, average=\"macro\"),\n",
        "            f1_score(y_test, y_pred, average=\"weighted\"),\n",
        "            f1_score(y_test, y_pred, average=\"macro\"),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "metrics_df[\"Score\"] = metrics_df[\"Score\"].round(4)\n",
        "metrics_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "per_class_df = (\n",
        "    pd.DataFrame(report)\n",
        "    .transpose()\n",
        "    .drop(index=[\"accuracy\", \"macro avg\", \"weighted avg\"])\n",
        "    .reset_index()\n",
        "    .rename(columns={\"index\": \"Cover_Type\"})\n",
        ")\n",
        "\n",
        "per_class_df[\"Cover_Type\"] = per_class_df[\"Cover_Type\"].astype(int)\n",
        "per_class_df[[\"precision\", \"recall\", \"f1-score\"]] = per_class_df[[\"precision\", \"recall\", \"f1-score\"]].round(4)\n",
        "per_class_df[\"support\"] = per_class_df[\"support\"].astype(int)\n",
        "per_class_df.sort_values(\"Cover_Type\", inplace=True)\n",
        "per_class_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "importances = pd.Series(rf_model.feature_importances_, index=X_test.columns)\n",
        "\n",
        "top_n = 15\n",
        "subset = importances.sort_values(ascending=False).head(top_n)\n",
        "\n",
        "plt.figure(figsize=(9, 6))\n",
        "sns.barplot(x=subset.values, y=subset.index, orient=\"h\", palette=\"viridis\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(f\"Top {top_n} Feature Importances\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tables above capture overall scoring and per-class performance, while the plots highlight where predictions concentrate and which features drive the model.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
